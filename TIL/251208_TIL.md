## 오늘 배운 것

- Deep learning basic theory
- CNN(Convolution Neural Network)

 Input - Layer1[Conv - Relu - Pool] - Layer2 - Flatten - FC -Output

- Relu(Rectified Linear Unit)
 - ReLU는 입력 값이 0보다 크면 그대로 출력하고, 0 이하이면 0을 출력하는 활성화 함수
- Tensor(다차원 배열)
- Time Series Data
- Parameter
 - weight(가중치): 입력 결정값
 - bias(편향): 출력 조정
- Neuron == 함수 
 - 이전 Layer의 모든 출력값을 0~1사이 수로 출력해주는 함
    입력 a₁ — W₁ ──┐
    입력 a₂ — W₂ ──┤ →  W·a + b → activation → 출력
    입력 a₃ — W₃ ──┘
- RNN(Recurrent Neural Network)
 - hidden state를 통해 이전시점의 위치정보 다음으로 넘겨줌

## 내일 할 일

- deep learning
- python basic skill

## Issue
